import shutil
import numpy as np
from settings import Data
import argparse
import heapq


def word_efficiency_measure():
    """
    For a word whose index is x, we calculate first calculate
    S(x) = \sum\limits_{i=0}^{label_number - 1}
        (p_label(i) * p_word_conditioned[i][x])
    P(x in i) = p_label(i) * p_word_conditioned[i][x] / S
    H(x) = \sum\limits_{i=0}^{label_number - 1}
        -P(x in i) * log(P(x in i))
    The smaller the H(x) is, more effective is the word
    whose index is x.
    The return value is -H(x).

    :return: an array which contains efficiency measure of each word
    """
    p_word_conditioned_temp = np.zeros([Data.label_number, Data.vocabulary_size])
    # S(x) = \sum\limits_{i=0}^{label_number - 1}
    # (p_label(i) * p_word_conditioned[i][x])
    for i in range(Data.label_number):
        p_word_conditioned_temp[i] \
            = Data.p_word_conditioned[i] \
              * Data.p_class[i]
    word_sum = np.sum(p_word_conditioned_temp, axis=0)
    # P(x in i) = p_label(i) * p_word_conditioned[i][x] / S
    p_word_in_each_class = p_word_conditioned_temp / word_sum
    # Effectiveness is the opposite number of entropy
    # As a smaller entropy means a larger effectiveness, we use
    # the opposite number of entropy as indicator of effectiveness
    effectiveness = np.sum(p_word_in_each_class * np.log(p_word_in_each_class), axis=0)
    np.savetxt("word_efficiency", effectiveness)
    return effectiveness


def classify_data():
    """
    Classify each file according to formula .
    :return: a vector of labels of all test files.
    """
    test_label = np.zeros(Data.test_file_number, dtype=int)
    test_data = open("data/test.data", 'r')
    posterior = Data.posterior
    p_word = Data.p_word_conditioned
    posterior_current = np.copy(posterior)
    current_file_index = 1
    # Add logarithm of possibilities for each file,
    # and label each file with the class that
    # produces the largest sum
    for line in test_data:
        data = map(int, line.strip().split(" "))
        if data:
            # if the file index doesn't change,
            # continue to add possibilities of this
            # file
            if current_file_index == data[0]:
                posterior_current \
                    += np.log(p_word[:, data[1] - 1]) * data[2]
            # otherwise find the largest possibility and label this file
            # with that possibility
            else:
                test_label[current_file_index - 1] \
                    = np.argmax(posterior_current)
                current_file_index = data[0]
                posterior_current = np.copy(posterior)
        # data == empty means that we have reached the end of the file,
        # and thus we should select the class with the largest possibility
        else:
            test_label[current_file_index - 1] \
                = np.argmax(posterior_current)
    # let test_label start from 1
    return test_label + 1


def cal_confusion_matrix(label_result):
    """
    Generate confusion matrix according to the classification result and
    actual labels
    :param label_result: The classification result generated by this program
    :return: confusion matrix
    """
    matrix = np.zeros([Data.label_number, Data.label_number], dtype=int)
    for i in np.arange(Data.test_file_number):
        # first index means the actual label
        # second index means the predicted label
        matrix[Data.test_label[i] - 1][label_result[i] - 1] += 1
    return matrix


def accuracy(label_result):
    """
    Calculate accuracy of this program on test data.
    :param label_result: The label result given by this program on
                         test data.
    :return: accuracy
    """
    return np.sum(label_result == Data.test_label) / \
           float(Data.test_file_number)


def output_in_tex(confusion_matrix):
    """
    Output a file "confusionMatrix" which contains the confusion
    matrix partially in format of latex and an array which contains
    the accuracy of classification for each class of texts
    :param confusion_matrix: the confusion matrix generated
                             by method cal_confusion_matrix()
    :return: null
    """
    # Calculate the classification accuracy for each class
    class_accuracy = np.zeros(Data.label_number)
    class_number = map(float, np.sum(confusion_matrix, axis=1))
    for i in range(Data.label_number):
        class_accuracy[i] = confusion_matrix[i][i] / class_number[i]
    category_accuracy_file = open("categorical_accuracy", 'w')
    # Output the classification accuracy vector into a file
    # in format of latex
    for i in range(Data.label_number):
        print >> category_accuracy_file, "&%.2f" % class_accuracy[i],
    category_accuracy_file.close()

    # Output the confusion matrix into the file confusionMatrix
    matrix_file = open("confusionMatrix", 'w')
    # Output the first line of the table
    for i in range(Data.label_number):
        print >> matrix_file, "&" + str(i + 1),
    print >> matrix_file, ""
    # Output the content of the file in the format &x&x\\\n
    for i in range(Data.label_number):
        print >> matrix_file, i + 1,
        for j in range(Data.label_number):
            print >> matrix_file, "&" + str(confusion_matrix[i][j]),
        print >> matrix_file, "\\\\"


def best_n_words(n):
    """
    This method finds the top N words which
    the classifier mostly relies on.
    :param n: number of words returned
    :return: a list of indices of the top N words which
    the classifier mostly relies on.
    """
    effectiveness = word_efficiency_measure()
    heap = []
    vocabulary = np.loadtxt("data/vocabulary.txt", dtype='string')
    # Push the first 100 element into the heap.
    for i in range(n):
        heapq.heappush(heap, (effectiveness[i], vocabulary[i], i))
    # Add the rest of elements to the heap but
    # remove the smallest before that.
    for i in np.arange(n, len(effectiveness)):
        heapq.heappushpop(heap, (effectiveness[i], vocabulary[i], i))
    top_words = []
    top_words_count = np.zeros([n, Data.label_number])
    for i in range(n):
        heap_tuple = heapq.heappop(heap)
        top_words.insert(0, heap_tuple[1])
        # record the occurrences of the word in each category of texts
        top_words_count[n - 1 - i, :] = Data.word_count_conditioned[:, heap_tuple[2]]
    np.savetxt("TopWords", top_words, "%s")
    np.savetxt("TopWordsCountsInDifferentCategories", top_words_count, "%d")


def main():
    Data()
    # Gauge mode, find the relation between pseudo counter and accuracy
    if Data.gauge:
        file_name = "betaAccuracyRelation" + \
                   str(Data.gauge_start) + \
                   "-" + str(Data.gauge_end) + \
                   "-" + str(Data.gauge_number)
        beta_accuracy_relation = open(file_name, 'w')
        range_beta = np.logspace(Data.gauge_start,
                                 Data.gauge_end, num=Data.gauge_number)
        for beta in range_beta:
            Data.dirichlet_para = beta
            # recalculate p_word_conditioned whenever dirichlet_para changes
            Data.p_word_conditioned = Data.possibility_of_word()
            result = classify_data()
            accuracy_classification = accuracy(result)
            print "%.2e %f" % (beta, accuracy_classification)
            print >> beta_accuracy_relation, \
                "%.2e %f" % (beta, accuracy_classification)
        beta_accuracy_relation.close()
        shutil.copy(file_name, ".//betaAccuracyRelationFile")
    # normal mode
    else:
        result = classify_data()
        np.savetxt("classification_result.txt", result, fmt="%d")
        print accuracy(result)
        if Data.find_best_words:
            best_n_words(100)
        if Data.output_confusion_matrix:
            con_matrix = cal_confusion_matrix(result)
            output_in_tex(con_matrix)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("-g", "--gaugeMode", action="store_true",
                        help="If flag g is used, the program runs in gauge mode and finds"
                             "relation between pseudo counter and accuracy."
                             "-m and -t will not take effect when -g is used."
                             "The gauge result is output in betaAccuracyRelation-"
                             "s-e-n, where s, e, n represents start, end, number"
                             "respectively. The result is also output into"
                             "betaAccuracyRelationFile.")

    parser.add_argument("-s", "--start", type=float, default=-5,
                        help="Set the starting point for para gauge."
                             "Default value is -5, which corresponds to"
                             "1e-5")

    parser.add_argument("-e", "--end", type=float, default=0,
                        help="Set the ending point for para gauge."
                             "Default value is 0, which corresponds to 1")

    parser.add_argument("-n", "--number", type=int, default=50,
                        help="Set the number of sample points for para gauge."
                             "The sample points distributes in correspondence to"
                             "logarithm space. Default value is 50")

    parser.add_argument("-b", "--beta", type=float, default=(1. / Data.vocabulary_size),
                        help="Set the parameter for additive smoothing,"
                             "default value is 1 / vocabulary_size")

    parser.add_argument("-m", "--confusionMatrix", action="store_true",
                        help="If flag m is used, the program outputs"
                             " confusion matrix in partial format of latex."
                             "The output file is named confusionMatrix."
                             "This flag also outputs classification accuracy"
                             "of each category in file categorical_accuracy")

    parser.add_argument("-t", "--best_words", action="store_true",
                        help="If flag t is used, the program outputs "
                             "a list of words that it mostly relies on in "
                             "file TopWords. It also outputs a file named "
                             "TopWordsCountsInDifferentCategories which"
                             " contains how many key words each category contains")

    args = parser.parse_args()
    Data.gauge_end = args.end
    Data.gauge_start = args.start
    Data.gauge_number = args.number
    Data.dirichlet_para = args.beta
    Data.gauge = args.gaugeMode
    Data.output_confusion_matrix = args.confusionMatrix
    Data.find_best_words = args.best_words
    main()
